

```markdown
# ğŸ§  AlpaCare â€” Medical Instruction Assistant

ğŸ“¦ **GitHub Repository:** [https://github.com/atharv-17-L/AlpaCare](https://github.com/atharv-17-L/AlpaCare)

---

## ğŸ“˜ Overview

**AlpaCare** is a fine-tuned **DistilGPT-2** language model built to provide **safe and educational medical guidance**.  
It uses **LoRA (Low-Rank Adaptation)** via **PEFT** for lightweight and efficient fine-tuning on the **AlpaCare-MedInstruct-52k** dataset from Hugging Face.  

The project demonstrates how small transformer models can be adapted for **domain-specific, safe response generation** in healthcare contexts.

> âš ï¸ **Disclaimer:** This model is for educational purposes only and does not provide medical advice.

---

## ğŸ§© Key Features

- ğŸ§  Fine-tunes **DistilGPT-2** using **LoRA (PEFT)**  
- âš™ï¸ Implemented completely on **Google Colab**  
- ğŸ—‚ï¸ Dataset: `lavita/AlpaCare-MedInstruct-52k`  
- ğŸ’¾ Produces a reusable **LoRA adapter**  
- âš¡ Lightweight, low-cost fine-tuning  
- ğŸ” Includes disclaimers for medical safety  

---

## ğŸ“ Repository Structure

```

AlpaCare/
â”‚
â”œâ”€â”€ data_loader.py                 # Loads and preprocesses dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ colab_finetune.ipynb       # Fine-tuning notebook
â”‚   â””â”€â”€ inference_demo.ipynb       # Inference and testing notebook
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ train-00000.parquet        # Source dataset file (sample)
â”‚
â”œâ”€â”€ lora_adapter.zip               # Trained LoRA adapter (model weights)
â”‚
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt               # Dependencies list

````

---

## âš™ï¸ Setup and Execution Guide

You can run this project on **Google Colab** (recommended) or locally on your computer.

---

### ğŸ§­ Option 1 â€” Run on Google Colab (Recommended)

1. Open the notebook:  
   ğŸ‘‰ [`notebooks/colab_finetune.ipynb`](./notebooks/colab_finetune.ipynb)
2. Run all cells step-by-step:
   - Install dependencies  
   - Load dataset (`data_loader.py`)  
   - Load tokenizer and model (`DistilGPT-2`)  
   - Configure **LoRA (r=8, Î±=32, dropout=0.1)**  
   - Train and save adapter  
3. After training, download the generated folder **`lora_adapter`** for inference.

---

### ğŸ§­ Option 2 â€” Run Locally (on Your PC)

#### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/atharv-17-L/AlpaCare.git
cd AlpaCare
````

#### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate       # On Windows: venv\Scripts\activate
```

#### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

#### 4ï¸âƒ£ Launch the Notebook

Open and run:

```
notebooks/colab_finetune.ipynb
```

---

## ğŸ§® How to Run Inference

After fine-tuning, open **`inference_demo.ipynb`** and run:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

base_model = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)
model = PeftModel.from_pretrained(model, "./lora_adapter")

def generate_response(prompt):
    input_text = f"Instruction: {prompt}\n\nResponse:"
    inputs = tokenizer(input_text, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=150)
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text + "\n\nâš ï¸ Disclaimer: Educational only. Consult a clinician."

print(generate_response("What are healthy bedtime habits?"))
```

---

## ğŸ“Š Sample Input / Output
AlpaCare/
â”‚
â”œâ”€â”€ data_loader.py # Loads and preprocesses the dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ colab_finetune.ipynb # Fine-tuning notebook
â”‚ â””â”€â”€ inference_demo.ipynb # Inference and testing notebook
â”‚
â”œâ”€â”€ dataset/
â”‚ â””â”€â”€ train-00000.parquet # Sample dataset file
â”‚
â”œâ”€â”€ lora_adapter.zip # Trained LoRA adapter (model weights)
â”‚
â”œâ”€â”€ LICENSE # MIT License
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ requirements.txt # Python dependencies list

---

## ğŸ§  LoRA Configuration Details

| Parameter |   Value   | Description          |
| :-------- | :-------: | :------------------- |
| Rank (r)  |     8     | Adapter layer rank   |
| Alpha (Î±) |     32    | Scaling factor       |
| Dropout   |    0.1    | Prevents overfitting |
| Task Type | Causal LM | Text generation task |

ğŸ’¡ These hyperparameters ensure efficient and memory-friendly fine-tuning suitable for Google Colab GPUs.

---

## ğŸ“ˆ Evaluation

* âœ… Human evaluation: 30 outputs tested
* âœ… Metrics: *Clarity*, *Safety*, *Relevance*, *Fluency*
* âš™ï¸ Optional automatic metrics: BLEU, Rouge
* ğŸ§¾ Results summarized in project report

---

## ğŸ§¾ Dependencies

All dependencies are in `requirements.txt`.
Or install manually:

```bash
pip install transformers datasets peft loralib torch pandas pyarrow accelerate scikit-learn
```

---

## ğŸ›¡ï¸ Ethical & Safety Statement

* The model **does not provide medical advice**.
* Every output includes a **medical disclaimer**.
* Built using ethical AI guidelines from Hugging Face.

---

## ğŸ“˜ References

* Dataset: [lavita/AlpaCare-MedInstruct-52k](https://huggingface.co/datasets/lavita/AlpaCare-MedInstruct-52k)
* Model: [DistilGPT-2](https://huggingface.co/distilgpt2)
* Frameworks: Hugging Face Transformers, PEFT, Datasets, Accelerate

---

## ğŸ“„ License

This project is released under the **Apache 2.0** â€” free for educational and research use only.

---

## ğŸ‘¤ Author

**Atharv Latta**
Solar Industries India Ltd â€” AIML Internship Assessment (2025)

---

## âœ… Submission Checklist

| Item                  |   Status  |
| --------------------- | :-------: |
| Public GitHub Repo    |     âœ…     |
| README.md (Detailed)  |     âœ…     |
| LoRA Adapter Uploaded |     âœ…     |
| Fine-tuning Notebook  |     âœ…     |
| Inference Notebook    |     âœ…     |
| Dataset (â‰¤100 MB)     |     âœ…     |
| License               |     âœ…     |
| JotForm Submission    | â³ Pending |

```

---


---

Would you like me to now generate a **matching `requirements.txt`** file (so you can upload that too right away)?
```
